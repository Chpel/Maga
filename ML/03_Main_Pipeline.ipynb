{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метрики классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  score  class\n",
       "0   1    0.5      0\n",
       "1   2    0.1      0\n",
       "2   3    0.2      0\n",
       "3   4    0.6      1\n",
       "4   5    0.2      1\n",
       "5   6    0.3      1\n",
       "6   7    0.0      0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"id\": list(range(1, 8)),\n",
    "    \"score\": [0.5, 0.1, 0.2, 0.6, 0.2, 0.3, 0.0], # так обычно выглядит выход из бинарного классификатора\n",
    "    \"class\": [0, 0, 0, 1, 1, 1, 0] # \"правильные\" метки\n",
    "})\n",
    "\n",
    "df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А как понять, насколько хороши эти предсказания?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TP, TN, FP, FN, confusion matrix\n",
    "\n",
    "В бинарной классификации для каждого наблюдаемого бывает всего 4 ситуации, для каждой из (по историческим причинам) есть свои названия.\n",
    "\n",
    "True Positive - предсказано 1, метка -- 1\n",
    "\n",
    "True Negative - предсказано 0, метка -- 0\n",
    "\n",
    "False Positive - предсказано 1, метка -- 0\n",
    "\n",
    "False Negative - предсказано 0, метка -- 1\n",
    "\n",
    "### Задание: давайте вспомним, что такое accuracy, что такое точность (precision) и полнота (recall)\n",
    "\n",
    "**Accuracy** -- доля верно угаданных; то есть доля совпадающих значений в последних двух колонках\n",
    "\n",
    "**Recall** -- доля определённого первого класса среди истинного первого класса; то есть, сколько ИЗВЛЕКЛИ из нужного класса\n",
    "\n",
    "**Precision** -- доля правильных среди предсказанных как первый класс\n",
    "\n",
    "**F1-score** -- среднее гармоническое точности и полноты\n",
    "\n",
    "Как выразить эти метрики через TP, TN, FP, FN ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# это уже не только для бинарно классификации, можно построить confusion matrix для любого числа классов\n",
    "sns.heatmap(\n",
    "    confusion_matrix(df['score'] > 0.5, # а почему такой трешхолд?\n",
    "                     df['class']), \n",
    "    annot=True, \n",
    "    fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "df[\">2\"] = df[\"score\"].map(lambda x: 1 if x >= 0.2 else 0)\n",
    "df#.drop(\"score\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = 3 / 5\n",
    "assert precision == precision_score(df[\"class\"], df[\">2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = 3 / 3\n",
    "assert recall == recall_score(df[\"class\"], df[\">2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 5 / 7 == accuracy_score(df[\"class\"], df[\">2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = 2* precision * recall / (precision + recall)\n",
    "assert f1 == f1_score(df[\"class\"], df[\">2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC [Reciever Operating Characteristic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(df[\"class\"], df[\"score\"])\n",
    "roc_auc = roc_auc_score(df[\"class\"], df[\"score\"])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "lw = 4 # line width\n",
    "\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.4f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC example by A. D.')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in list(thresholds):\n",
    "    TP = df[(df[\"score\"] >= t) & (df['class'] == 1)]['id'].count()\n",
    "    TN = df[(df[\"score\"] < t) & (df['class'] == 0)]['id'].count()\n",
    "    FP = df[(df[\"score\"] >= t) & (df['class'] == 0)]['id'].count()\n",
    "    FN = df[(df[\"score\"] < t) & (df['class'] == 1)]['id'].count()\n",
    "    TP_rate = TP / (TP + FN)\n",
    "    FP_rate = FP / (TN + FP)\n",
    "    print(f\" Threshold: {t} TP Rate: {TP_rate:6f} FP Rate: {FP_rate}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr, fpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но вообще есть `plot_roc_curve` и вот такое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "RocCurveDisplay(fpr=fpr, tpr=tpr).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall curve\n",
    "\n",
    "Примерно такой же интерфейс. Кривая точность-полнота может быть довольно-таки бесноватой по сравнению с ROC curve; частый случай -- \"зубцы\", сейчас мы в этом убедимся."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(df[\"class\"], df[\"score\"])\n",
    "\n",
    "precision, recall, thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть и более прогрессивные способы сделать красиво"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "pr_display = PrecisionRecallDisplay(precision=precision, recall=recall).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Упражнение. Реализуйте сами график precision-recall кривой и посчитайте площадь под ней"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# И ещё раз, без спешки\n",
    "\n",
    "Самое крупное кораблекрушение Европы в мирное время, в предполагаемых причинах которого до сих пор нет окончательной уверенности.\n",
    "\n",
    "```\n",
    "«Estonia» (ранее «Viking Sally», «Silja Star», «Wasa King») — \n",
    "эстонский паром судоходной компании «Estline», построенный \n",
    "в 1979 году в ФРГ на судоверфи «Meyer Werft» в Папенбурге. \n",
    "Затонул в Балтийском море в ночь с 27 на 28 сентября 1994 года, \n",
    "в результате крушения пропали без вести 757 человек и \n",
    "погибли 95 человек (всего 852) из 989 находившихся \n",
    "на борту пассажиров и членов экипажа. Это крупнейшее \n",
    "в Европе кораблекрушение в мирное время. \n",
    "```\n",
    "\n",
    "Датасет можно скачать [тут](https://www.kaggle.com/datasets/christianlillelund/passenger-list-for-the-estonia-ferry-disaster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_data = pd.read_csv(\"data/estonia-passenger-list.csv\")\n",
    "all_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"Country\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anonymized_data = all_data.drop([\"Firstname\", \"Lastname\", \"PassengerId\"], axis=1)\n",
    "anonymized_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = anonymized_data[\"Survived\"]\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anonymized_data[anonymized_data['Country'] == 'Belarus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ставим при необходимости библиотеку прямо из ноутбука"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# нарисуем среднюю выживаемость и дисперсию\n",
    "sns.barplot(x='Country',y='Survived', data=anonymized_data)\n",
    "\n",
    "# Поворот  подписей\n",
    "plt.xticks(rotation=70)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Зависит ли выживаемость от категории Пассажиры/Passengers; Команда/Сrew \n",
    "sns.barplot(x='Category', y='Survived', data=anonymized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anonymized_data[\"Category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А как распределён возраст?\n",
    "\n",
    "Можно посмотреть встроенными средствами pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anonymized_data[\"Age\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных\n",
    "\n",
    "#### Dummy-coding AKA One-Hot-Encoding\n",
    "\n",
    "Но чаще всё-таки удобнее кодировать средствами sklearn, увидим ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = pd.get_dummies(anonymized_data[\"Country\"], prefix=\"c\")\n",
    "countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сами разобьём на обучающую и тестовую выборки -- до всех предобработок, это важно!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "data_train, data_test, y_train, y_test = train_test_split(\n",
    "                                                    anonymized_data.drop([\"Survived\"], axis=1), # X\n",
    "                                                    anonymized_data[\"Survived\"], # y\n",
    "                                                    test_size=0.3, # доля от всех записей\n",
    "                                                    random_state=1337, # зерно\n",
    "                                                    stratify=anonymized_data[\"Survived\"], # а это что?\n",
    "                                        )\n",
    "\n",
    "# print(data_train.shape, y_train.shape, data_test.shape, y_test.shape)\n",
    "\n",
    "# np.sum(y_train) / y_train.shape[0], np.sum(y_test) / y_test.shape[0]\n",
    "# nonames_data.shape, data_train.shape\n",
    "# y_train, y_test\n",
    "# y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import sparse as sp\n",
    "\n",
    "# Не все классификаторы умеют обращаться с категориальными признаками. \n",
    "def prepare_features_for_logreg(data: pd.DataFrame, cat_encoder=None, real_scaler=None):\n",
    "    cat_columns = [\"Country\", \"Sex\", \"Category\"]  \n",
    "    real_columns = [\"Age\"]\n",
    "    \n",
    "    # categorical features\n",
    "    if cat_encoder is None:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "        ohe.fit(data[cat_columns])\n",
    "    else:\n",
    "        ohe = cat_encoder\n",
    "    X_cat = ohe.transform(data[cat_columns])\n",
    "    cat_fnames = ohe.get_feature_names_out(cat_columns)\n",
    "    \n",
    "    # real-valued features\n",
    "    if real_scaler is None:\n",
    "        stsc = StandardScaler()\n",
    "        stsc.fit(data[real_columns])\n",
    "    else:\n",
    "        stsc = real_scaler\n",
    "    X_real = stsc.transform(data[real_columns])\n",
    "    feature_matrix = sp.hstack([X_cat, X_real])\n",
    "    \n",
    "    return feature_matrix, list(cat_fnames) + real_columns, ohe, stsc\n",
    "\n",
    "X_train_sparse, fnames_sparse, encoder_sparse, scaler = prepare_features_for_logreg(data_train)\n",
    "X_test_sparse, _, _, _ = prepare_features_for_logreg(data_test, encoder_sparse, scaler)\n",
    "\n",
    "X_train_sparse.shape, X_test_sparse.shape\n",
    "# X_train_sparse.todense()\n",
    "# X_test_sparse.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_linear = LogisticRegression(C=0.99, class_weight=\"balanced\", \n",
    "                                solver=\"saga\", penalty=\"l1\")\n",
    "clf_linear.fit(X_train_sparse, y_train)\n",
    "\n",
    "y_pred = clf_linear.predict(X_test_sparse)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "# clf_linear.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "\n",
    "eli5.explain_weights(clf_linear, feature_names=fnames_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хороший способ проверить, не ерунду ли мы сделали: внимание на **accuracy** и на метрики в разделе **macro-averaged**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "clf_dummy = DummyClassifier(strategy=\"most_frequent\").fit(X_train_sparse, y_train)\n",
    "y_pred = clf_dummy.predict(X_test_sparse)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим признаки для логических классификаторов -- там обычно можно без разреженных признаков и нормализации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from scipy import sparse as sp\n",
    "\n",
    "def prepare_features_for_logic(data: pd.DataFrame, cat_encoder=None):\n",
    "    \n",
    "    cat_columns = [\"Country\", \"Sex\", \"Category\"]  \n",
    "    real_columns = [\"Age\"]\n",
    "    \n",
    "    # categorical features\n",
    "    \n",
    "    if cat_encoder is None:\n",
    "        oe = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "        oe.fit(data[cat_columns])\n",
    "    else:\n",
    "        oe = cat_encoder\n",
    "    \n",
    "    X_cat = oe.transform(data[cat_columns])\n",
    "    mapped_cat_values = oe.categories_\n",
    "    cat_fnames = cat_columns\n",
    "    \n",
    "    # real-valued features\n",
    "    \n",
    "    # todo: вообще очень часто есть смысл отбросить из обучающей выборки примеры, \n",
    "    #       значения которых редки (например, выпадающие далеко \"за три сигмы\")\n",
    "    \n",
    "    X_real = data[real_columns].values    \n",
    "    feature_matrix = np.hstack([X_cat, X_real]) # note: `np` for dense Numpy matrices\n",
    "    \n",
    "    return feature_matrix, list(cat_fnames) + real_columns, oe, mapped_cat_values\n",
    "\n",
    "X_train_dense, fnames_dense, encoder, mapped_cat_values = prepare_features_for_logic(data_train)\n",
    "X_test_dense, _, _, _ = prepare_features_for_logic(data_test, encoder)\n",
    "\n",
    "\n",
    "X_train_dense.shape, X_test_dense.shape\n",
    "\n",
    "mapped_cat_values\n",
    "# X_train_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=2, class_weight=\"balanced\").fit(X_train_dense, y_train)\n",
    "y_pred = clf.predict(X_test_dense)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но пока мы даже не попытались настроить модели, результаты ничего не значат.\n",
    "\n",
    "Кстати, смотрите, как можно.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(12, 8), dpi=80)\n",
    "\n",
    "\n",
    "plot_tree(clf, feature_names=fnames_dense, class_names=[\"Dead\", \"Surv\"], proportion=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain Like I'm Five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "\n",
    "eli5.explain_weights(clf, feature_names=fnames_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# Познакомились с minimum minimorum, поперебираем\n",
    "попробуем получить результаты получше со случайным лесом\n",
    "\n",
    "Посмотрим на самые важные параметры.\n",
    "\n",
    "```\n",
    "class sklearn.ensemble.RandomForestClassifier(\n",
    "\n",
    "                        n_estimators=100,  \n",
    "                            Число деревьев.              \n",
    "                            \n",
    "                        criterion='gini', \n",
    "                            Критерий: индекс Джини либо энтропия, может зависеть от вида дерева\n",
    "                            \n",
    "                        max_depth=None, \n",
    "                            Самая, пожалуй, естественная регуляризация -- ограничение глубины дерева\n",
    "                            \n",
    "                        min_samples_split=2,\n",
    "                            Сколько должно попасть в вершину объектов, чтобы её можно было ветвить дальше\n",
    "                            \n",
    "                        min_samples_leaf=1, \n",
    "                            Очень мощный и важный регуляризатор! Сколько минимум объектов должно быть в листе\n",
    "                            \n",
    "                        max_features='auto', \n",
    "                            Среди какого числа признаков выбираем очередное ветвление\n",
    "                            \n",
    "                        max_leaf_nodes=None, \n",
    "                            Хороший регуляризатор -- ограничение на количество листьев; добавляются по убыванию\n",
    "                            снижения impurity.\n",
    "                            \n",
    "                        min_impurity_decrease=0.0, \n",
    "                            Порог по уменьшению impurity, который запрещает ветвить дерево дальше.\n",
    "                        \n",
    "                        bootstrap=True, \n",
    "                            Если False, обучаем каждое дерево на всём наборе данных. \n",
    "                            Если True, только на части, размер которой задан в max_samples.\n",
    "                            \n",
    "                        n_jobs=None, \n",
    "                            На сколько job-ов распараллелить.\n",
    "                            \n",
    "                        random_state=None, \n",
    "                            Ну, тут всё понятно: случайный seed, позволяющий воспроизводить результаты.\n",
    "                        \n",
    "                        verbose=0, \n",
    "                            Степень подробности протоколирования хода обучения и всего такого. Обычно 0,1,2.\n",
    "                            \n",
    "                        warm_start=False, \n",
    "                            Это такая возможность переиспользовать обученный ансамбль для последующих задач.\n",
    "                             \n",
    "                        max_samples=None\n",
    "                            Сколько максимум сэмплов брать из датасета для обучения очередного дерева, если bootstrap\n",
    "                        )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"n_estimators\": [10, 50, 100],\n",
    "    \"min_samples_leaf\": [1, 2, 3, 5],\n",
    "    \"max_samples\": [0.3, None],\n",
    "    \"class_weight\" : [\"balanced\", \"balanced_subsample\"]\n",
    "}\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape, y_test.shape, type(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Упражнение\n",
    "Зачем нужен KFold? Почему нельзя просто считать метрики на тесте?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=3, random_state=100, shuffle=True)\n",
    "\n",
    "print(y_train.values.mean())\n",
    "\n",
    "for array1, array2 in kfold.split(X_train_dense, y_train):   \n",
    "    x_train_cv = X_train_dense[array1]\n",
    "    y_train_cv = np.array(y_train)[list(array1)]\n",
    "    \n",
    "    x_test_cv = X_train_dense[array2]\n",
    "    y_test_cv = np.array(y_train)[list(array2)]\n",
    "    \n",
    "    print(y_train_cv.mean(), y_test_cv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "scores = [\"accuracy\"]\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning for %s\" % score)\n",
    "    print()\n",
    "    # loo = LeaveOneOut()\n",
    "\n",
    "    clf = GridSearchCV(RandomForestClassifier(n_jobs=-1, random_state=100), \n",
    "                       param_grid, scoring=score, verbose=1, cv=3)\n",
    "\n",
    "    clf.fit(X_train_dense, y_train)\n",
    "\n",
    "    print(\"Best params on dev set:\")\n",
    "    print(clf.best_params_)\n",
    "    \n",
    "    print(\"Scores on development set:\")\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "    \n",
    "    best_model = clf.best_estimator_\n",
    "    best_model.fit(X_train_dense, y_train)\n",
    "\n",
    "    y_true, y_pred = y_test, best_model.predict(X_test_dense)\n",
    "    \n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "\n",
    "eli5.explain_weights(best_model, feature_names=fnames_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Укладываем вообще всё в один пайплайн\n",
    "---\n",
    "Результаты вряд ли будут впечатляющими, но на этом примере посмотрим, как можно удобно запрограммировать перебор параметров не только классификации.\n",
    "\n",
    "Понизим размерность и применим KNN.\n",
    "\n",
    "Вариантов понижения размерности [много](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition). Возьмём самый стандартный и работающий с разреженными признаками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "decomposer = TruncatedSVD(n_components=6, random_state=10, n_iter=200)\n",
    "X_train_svd = decomposer.fit_transform(X_train_sparse)\n",
    "X_test_svd = decomposer.transform(X_test_sparse)\n",
    "\n",
    "X_train_svd.shape, X_test_svd.shape, X_train_sparse.shape, X_train_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline(steps=[('svd', decomposer), \n",
    "                       ('knn', knn)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "        \"svd__n_components\": [2, 4, 6, 10],\n",
    "        \"svd__n_iter\": [5, 100, 1000],\n",
    "        \"knn__n_neighbors\": [1, 2, 3, 4, 5],\n",
    "        \"knn__weights\" : [\"uniform\", \"distance\"],\n",
    "        \"knn__metric\" : [\"euclidean\"]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "scores = [\"f1_macro\"]#, \"accuracy\"]\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning for %s\" % score)\n",
    "    print()\n",
    "    \n",
    "    clf = GridSearchCV(pipe, param_grid, scoring=score, verbose=2, cv=3)\n",
    "    clf.fit(X_train_sparse, y_train)\n",
    "\n",
    "    print(\"Best params on dev set:\")\n",
    "    print(clf.best_params_)\n",
    "    \n",
    "    print(\"Scores on development set:\")\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "    \n",
    "    best_model = clf.best_estimator_\n",
    "    best_model.fit(X_train_sparse, y_train)\n",
    "\n",
    "    y_true, y_pred = y_test, best_model.predict(X_test_sparse)\n",
    "    \n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание со звездочкой 1\n",
    "\n",
    "Получите accuracy > 0.89 на тестовом датасете. Можно пользоваться любым классификатором **из sklearn**. Ансамблями пользоваться можно.\n",
    "\n",
    "### XtreemeGradientBoosting и нейронные сети запрещены. \n",
    "\n",
    "Выполняйте это задание в отдельном ноутбуке. Если у Вас получилось набрать нужный accuracy, то можете прислать его мне на проверку. Критeрии того, чтобы я его начал смотреть:\n",
    "\n",
    "* Ваш код читаем и прокоментирован, нет серьезных противоречий PEP8\n",
    "* Ваш ноутбук запускается и нигде не падает по кнопке \"Run all\". \n",
    "* В последней ячейке Вашего кода есть classification_report с accuracy > 0.89"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание со звездочкой 2\n",
    "\n",
    "Допустим, у вас есть два множества **A** и **B** точек на плоскости. Линейная регрессия на плоскости -- это \n",
    "просто прямая, ее можно представить как функцию $ y = ax + b$.\n",
    "\n",
    "Линейная регрессия, обученная на множестве **А**, имеет коэффициент **$a > 0$**. То же самое верно и для линейной регрессии, обученной на множестве **B**. Правда ли, что если обучить линейную регрессию на множестве $A \\cup B$, то у полученной прямой коэффициент **a** будет больше 0?\n",
    "\n",
    "Если да, докажите. Если нет -- пришлите мне контрпример (jupyter notebook, в котором есть эти множества и линейные регрессии). Требования к ноутбуку можно найти в задаче выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
