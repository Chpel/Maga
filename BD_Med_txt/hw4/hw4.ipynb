{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading:\n",
    "A couple of papers this time, one with very strong “anti-black-box” sentiment, and another on ops AI (from our team):\n",
    "Interpretable AI: https://arxiv.org/pdf/1811.10154.pdf (was published in Nature MI)\n",
    "Operational AI: https://rdcu.be/b4ffC \n",
    "Answer the questions\n",
    "\n",
    "\n",
    "# Question 1: (1 pt)\n",
    "One can always accurately predict patient wait time from the...\n",
    "\n",
    "    Wait time of the previous patient\n",
    "    Current time \n",
    "    Patient age\n",
    "    None of 3 other options (Ответ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Question 2: (1 pt)\n",
    "One of the most basic ways to build predictive models is known as\n",
    " \n",
    "    Linear regression (Ответ)\n",
    "    Linear segmentation\n",
    "    Health regression\n",
    "    Grid search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: (1 pt)\n",
    "Adding more predictors to the predictive model will always produce significantly better prediction quality\n",
    "\n",
    "    True\n",
    "    False (Ответ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: (1 pt)\n",
    "Reducing machine learning model feature count is important because\n",
    "\n",
    "    It is more resistant to manual errors in the data\n",
    "    It can help exclude missing features\n",
    "    It can help create interpretable models (Ответ)\n",
    "    It is not really important\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Solve data problems \n",
    "\n",
    "Time for building more models! – and now, after learning how to find and engineer their features, we can really look into using the most efficient, interpretable predictions. Model Interpretability is particularly appreciated in healthcare, where we want to understand why certain thing happen. Selecting the best short models is one of the best approaches to interpretability.\n",
    "Getting the data: Please go to this “Data Challenge”  (Operational AI section) web page, \n",
    "https://medicalanalytics.group/operational-data-challenge/\n",
    "and click on the “Data only” link there to download the data.\n",
    "In the sheet, use F3 tab (dataset) ONLY: delete F1, F2 and F4, to load faster in Python. Also, drop all variables with names prefixed with x_ (these are the original timestamps, they make very little sense as model features).\n",
    "Wait will be our target variable we want to predict/model.\n",
    "The last sheet in this file explains the features – most of them were engineered, and you should understand how by now. But there are tons of them, and we have to sort this out.\n",
    "Fitting the model: We can explore different machine learning models, but let’s pick linear regression, since it takes little time to compute. So if Y is the Wait, and X – everything else, you can fit regression with this code:\n",
    "from sklearn import linear_model\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(X, Y)\n",
    "Ypred = model.predict(X) # use trained regression model to predict\n",
    "r = Y-Ypred # compute prediction error (residual)\n",
    "e = abs(r).mean() # compute model error\n",
    "\n",
    "To help you make sure that you stay on the right track, I provide a few Checkpoints after each question. The numerical values provided in multiple choices can be rounded – always pick the closest to your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Question 5:  (4 pts)\n",
    "Using sklearn snippet above, build a linear regression model to predict Wait from all other columns (excluding the “x_” columns, as I mentioned). Once the model is built, use it to compute the predicted wait value Ypred, and residual r (the difference between the true and predicted wait).\n",
    "\n",
    "Let’s define model error  as the average of its absolute residual, as shown above: abs(r).mean()\n",
    "\n",
    "Find this error value e, and write it down.\n",
    "\n",
    "What was the model error value ?\n",
    "\n",
    "    23.25\n",
    "    25.25\n",
    "    27.25\n",
    "    29.25\n",
    "    31.25\n",
    "\n",
    "Checkpoint: the median of absolute residual values, abs(r).median(), should be around 19.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6:  (2 pts)\n",
    "Using the code below , understand and run Python built-in feature selection algorithm: RFE (Recursive Feature Elimination). In this code, “model” is the same regression model as you declared and used in the previous code.\n",
    "# Run Python feature selection\n",
    "if True: # just in case I want to disable this part\n",
    "    print('\\n>Python feature selection:')\n",
    "    from sklearn.feature_selection import RFE\n",
    "    from itertools import compress\n",
    "    for nFeatures in range(1,4):\n",
    "        rfe = RFE(model, n_features_to_select=nFeatures)\n",
    "        X_rfe = rfe.fit_transform(X,Y) #transforming data using RFE   \n",
    "        #Fitting the data to model\n",
    "        model.fit(X_rfe,Y)\n",
    "        #print(rfe.support_)\n",
    "        #print(rfe.ranking_)\n",
    "        cols = list(compress( X.columns, rfe.support_))\n",
    "        model.fit(X[cols],Y)\n",
    "        e = abs(Y-model.predict(X[cols])).mean()\n",
    "        print(e, cols)\n",
    "  \n",
    "Write down its output. What was the best 3-feature model error value?\n",
    "\n",
    "    33.25\n",
    "    32.25\n",
    "    31.25\n",
    "    30.25\n",
    "    29.25\n",
    "Checkpoint: All three models should include CardiacCount \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7:  (6 pts)\n",
    "Can we always rely on built-in code? Let’s implement our own simplest best model selection as a fast-forward “greedy” algorithm. It should work like this:\n",
    "Find the best one-feature model (try all one-feature models, and select the one with the lowest error e). This is our best feature F1.\n",
    "Using F1 from the first step, try adding one more feature to it (from all features you have left), to find the best 2-feature model (F1, F2)\n",
    "Similarly, keep adding more features: F3, F4, F5 – to the features from the previous step\n",
    "What is the e=abs(r).mean() value for the best 3-feature model?\n",
    "\n",
    "    34.7\n",
    "    32.7\n",
    "    30.7\n",
    "    28.7\n",
    "    26.7\n",
    "Checkpoint: this model should contain LineCount0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Question 8:  (3 pts)\n",
    "Feature selection is often used to create shorter, faster models. Why do we need several dozen features, if we can achieve a significantly smaller error with a only few? \n",
    "Run your greedy algorithm to find best models with up to 15 features (adding them one by one, as explained).\n",
    "Using your greedy algorithm, what is the smallest number of features to get the error value under 24?\n",
    "\n",
    "    4\n",
    "    6\n",
    "    8\n",
    "    10\n",
    "    12\n",
    "Checkpoint: model error for 14 features is around 23.548\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Bonus (optional question)\n",
    "# Question 9:  (6 pts)\n",
    "The problem with simple feature selectors (RFE, “greedy”) is that they are fast, but suboptimal; they also produce a single model only. As a result, you can miss the best model; you will also miss other nearly-best models which can help interpret the outcome. \n",
    "So let’s try another extreme: “brute force”. Consider all possible combinations of three features (F1,F2, F3). What is the best abs(r).mean() you can achieve now?\n",
    "Clearly, you will have to use three nested loops for F1, F2 and F3. I strongly recommend saving all resulting models into a list. To do so, record each model as a tuple (error, F1, F2, F3), and keep appending those to the list of computed models. At the end, sort the list by the first tuple value, to see which model won, and which came close. \n",
    "Note, that this code will take a few minutes to run. So try it with 2-feature models first to make sure it runs well, and then add F3.\n",
    "What is the e=abs(r).mean() value for the best 3-feature model?\n",
    "\n",
    "    22\n",
    "    24\n",
    "    26\n",
    "    28\n",
    "    30\n",
    "\n",
    "Checkpoint: \n",
    "\n",
    "Note: You should see that this time we achieved a lower error compared to the 2- or 3- feature models from above. Moreover, you can see many models producing nearly optimal results. When you do data-science, always keep this in mind: the truth can be very multi-faceted, and one can learn a lot from these outcomes. \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
