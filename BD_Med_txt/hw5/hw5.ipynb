{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b21e3240-5d3e-4345-8b3d-289805a6ee6e",
   "metadata": {},
   "source": [
    "# Question 1: (1 pt)\n",
    "Machine learning models can improve clinical decision making because\n",
    "\n",
    "    1.\tThey can be easily programmed in many programming languages \n",
    "    2.\tThey do not require iterative computations\n",
    "    3.\tThey use decision trees\n",
    "    4.\tThey identify data-driven, objective decision logic (+)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257ea2ed-39d0-4f78-a8ee-ea62050fb0fb",
   "metadata": {},
   "source": [
    "# Question 2: (1 pt)\n",
    "By limiting the depth of the decision tree we make sure that \n",
    "\n",
    "    1.\tIt does not overfit the data (+)\n",
    "    2.\tIt uses as many features as possible\n",
    "    3.\tIt provides the best fit into the data\n",
    "    4.\tIt can be used with random forest models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f416ea-5e46-4ce4-bf78-d782513e167b",
   "metadata": {},
   "source": [
    "# Question 3: (1 pt)\n",
    "One of the main advantages of decision tree model is:\n",
    "\n",
    "    1.\tStability\n",
    "    2.\tUsing linear regression\n",
    "    3.\tRepresenting interpretable decision logic (+)\n",
    "    4.\tNot overfitting the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04867318-7b44-4cfd-a8c8-401b0563f29b",
   "metadata": {},
   "source": [
    "# Question 4: (1 pt)\n",
    "Forest models are built by \n",
    "    \n",
    "    1.\tAveraging linear regressions\n",
    "    2.\tAveraging decision trees (+)\n",
    "    3.\tAveraging randomly-chosen leaves from the same large decision tree\n",
    "    4.\tSelecting the largest tree from a randomly generated set of decision trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25c0c97-5faa-4cee-a013-5dc14639d262",
   "metadata": {},
   "source": [
    "# 2\tSolve data problems \n",
    "In this part, we will take a bit more time to explore the Wisconsin Breast Cancer dataset, described in the class. Load the dataset using the file provided with this homework . Use ‘diagnosis’ as your target variable Y to be predicted (you can replace Malignant by 1, and Benign by 0, thus predicting cancer malignancy). Make sure you remove ‘diagnosis’ and ‘id’ from the feature variables X.\n",
    "Note that there are many types of model accuracy metrics, they all can be found in sklearn.metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebcc0a5-15bc-4a0b-a973-fd7d1b194e5b",
   "metadata": {},
   "source": [
    "## Greedy model selection algorithm\n",
    "    1.\tFind the best one-feature model (try all one-feature models, and select the one with the lowest error e). This is our best feature F1.\n",
    "    2.\tUsing F1 from the first step, try adding one more feature to it (from all features you have left), to find the best 2-feature model (F1, F2)\n",
    "    3.\tSimilarly, keep adding more features: F3, F4, F5 – to the features from the previous step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2acf07e-fbdc-4ad4-a65a-d73765d202a0",
   "metadata": {},
   "source": [
    "# Question 5:  (5 pts)\n",
    "Model A\n",
    "Using LogisticRegression model and our greedy model selection algorithm that we have used previously (see the insert below), find the best 3-feature model. Use sklearn.metrics.accuracy_score  to select the model with the highest accuracy. Which one of the features below appears in this model?\n",
    "    \n",
    "    1.\tcompactness_worst \n",
    "    2.\tfractal_dimension_se\n",
    "    3.\tsmoothness_se \n",
    "    4.\tconcave points_mean\n",
    "    5.\tperimeter_worst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe365943-ef4b-47b2-8ebd-201e4081ee95",
   "metadata": {},
   "source": [
    "# Question 6:  (3 pts)\n",
    "Model B\n",
    "Predicting cancer malignancy, one would definitely want to avoid false negatives: malignant cancer cases erroneously classified by the model as benign. \n",
    "Recall (or Sensitivity, https://en.wikipedia.org/wiki/Precision_and_recall) is another quality metric, which determines how well a model can avoid false negatives. So let’s use the above greedy selection, and change model selection metric from accuracy to recall (sklearn.metrics.recall_score). \n",
    "Rerun the greedy code with this metric to select the 2-feature model with the highest recall. \n",
    "How many features in this new model are the same as in the model A above?\n",
    "\n",
    "    1.\tNone\n",
    "    2.\tOne\n",
    "    3.\tTwo\n",
    "    4.\tThree\n",
    "\n",
    "Checkpoint: Model recall score should be close to 0.919811\n",
    "Another interesting checkpoint – try running this algorithm to select the best 3-, 4-, 5-feature model, and you will see, that the best recall value is not changing; and going higher will definitely result in an overfit. So we have an interesting case when 2 features get all the job done.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1a6af7-602f-4ff5-891a-7898a31a56d9",
   "metadata": {},
   "source": [
    "# Question 7:  (6 pts)\n",
    "Model C\n",
    "Regression is great, but we want to find a very simple model any physician can use. So let’s consider building a decision tree model. To develop something very interpretable that can be used by humans, let’s also limit the number of decision tree leaves to 3 by using max_leaf_nodes parameter in DecisionTreeClassifier() . \n",
    "As always, train the model on the entire dataset. Make a plot of the resulting model tree:\n",
    "\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "        _ = tree.plot_tree(model, \n",
    "                           feature_names=list(X.columns),  \n",
    "                           class_names=['B','M'],\n",
    "                           rounded = True,\n",
    "                           fontsize = 12,\n",
    "                           filled=True)\n",
    "\n",
    "                           \n",
    "We have a patient with the following features\n",
    "radius_worst<12 \n",
    "radius_mean>9\n",
    "concave_points_worst<0.1\n",
    "\n",
    "Looking at this tree plot only, answer the following question:\n",
    "What does this model tell us about the patient cancer classification?\n",
    "\n",
    "    1.\tBenign\n",
    "    2.\tMalignant\n",
    "    3.\tEqually probable to be Benign or Malignant\n",
    "\n",
    "Checkpoint: Model accuracy score should be close to 0.94024\n",
    "\n",
    "3)  Do not use any other parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f712e9-e38a-4edd-8729-d1f5e9acc1ff",
   "metadata": {},
   "source": [
    "# Question 8:  (4 pts)\n",
    "Model D\n",
    "Now, let’s build a really small random forest, with only 10 trees, setting tree maximum depth  to 3. Compute the accuracy score of this forest (Model D) to the models A and C above. Which of the three models produced the most accurate result?\n",
    "\n",
    "    1.\tA\n",
    "    2.\tC\n",
    "    3.\tD\n",
    "    4.\tAll three models have the same accuracy\n",
    "    \n",
    "Note: Random forest uses random seeding to create its trees, so it may return slightly different results with each model run. Run it a few times comparing to the other models, and report most frequent result. \n",
    "\n",
    "4) Set these parameter values in RandomForestClassifier();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0cddb5-66c0-4434-8eb4-637355e0a3e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
